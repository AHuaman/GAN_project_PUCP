{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Generativas Adversarias: Modelo base\n",
    "\n",
    "En este notebook se presentará a ejecución del modelo base de las **Redes Generativas Adversarias** (GAN en sus siglas en inglés). El presente modelo está basado en la implementación en Tensorflow de \"Vanilla GAN\", ubicado en [este repositorio](https://github.com/wiseodd/generative-models). Dicha implementación corresponde al modelo original propuesto por un paper de [NIPS 2014 por Ian Goodfellow, et al.](http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf). Se utilizará la base de datos MNIST, utilizada por el Goodfellow et al. [2014] y disponible en la librería de tensorflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lo de abajo es una celda con el modelo completo original copiado. No correr**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-7f1a50426b13>:93: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../../MNIST_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../../MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../../MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Iter: 0\n",
      "D loss: 1.324\n",
      "G_loss: 2.692\n",
      "\n",
      "Iter: 1000\n",
      "D loss: 0.01821\n",
      "G_loss: 8.432\n",
      "\n",
      "Iter: 2000\n",
      "D loss: 0.01988\n",
      "G_loss: 7.47\n",
      "\n",
      "Iter: 3000\n",
      "D loss: 0.04085\n",
      "G_loss: 6.38\n",
      "\n",
      "Iter: 4000\n",
      "D loss: 0.04337\n",
      "G_loss: 7.131\n",
      "\n",
      "Iter: 5000\n",
      "D loss: 0.2898\n",
      "G_loss: 3.979\n",
      "\n",
      "Iter: 6000\n",
      "D loss: 0.4463\n",
      "G_loss: 3.563\n",
      "\n",
      "Iter: 7000\n",
      "D loss: 0.4018\n",
      "G_loss: 4.394\n",
      "\n",
      "Iter: 8000\n",
      "D loss: 0.389\n",
      "G_loss: 3.601\n",
      "\n",
      "Iter: 9000\n",
      "D loss: 0.4933\n",
      "G_loss: 2.998\n",
      "\n",
      "Iter: 10000\n",
      "D loss: 0.3028\n",
      "G_loss: 2.823\n",
      "\n",
      "Iter: 11000\n",
      "D loss: 0.5538\n",
      "G_loss: 2.926\n",
      "\n",
      "Iter: 12000\n",
      "D loss: 0.4618\n",
      "G_loss: 2.691\n",
      "\n",
      "Iter: 13000\n",
      "D loss: 0.4823\n",
      "G_loss: 3.24\n",
      "\n",
      "Iter: 14000\n",
      "D loss: 0.7148\n",
      "G_loss: 2.413\n",
      "\n",
      "Iter: 15000\n",
      "D loss: 0.625\n",
      "G_loss: 2.534\n",
      "\n",
      "Iter: 16000\n",
      "D loss: 0.542\n",
      "G_loss: 2.68\n",
      "\n",
      "Iter: 17000\n",
      "D loss: 0.5383\n",
      "G_loss: 2.594\n",
      "\n",
      "Iter: 18000\n",
      "D loss: 0.5176\n",
      "G_loss: 2.486\n",
      "\n",
      "Iter: 19000\n",
      "D loss: 0.6749\n",
      "G_loss: 2.184\n",
      "\n",
      "Iter: 20000\n",
      "D loss: 0.5182\n",
      "G_loss: 2.278\n",
      "\n",
      "Iter: 21000\n",
      "D loss: 0.5892\n",
      "G_loss: 2.63\n",
      "\n",
      "Iter: 22000\n",
      "D loss: 0.4783\n",
      "G_loss: 2.587\n",
      "\n",
      "Iter: 23000\n",
      "D loss: 0.6018\n",
      "G_loss: 2.606\n",
      "\n",
      "Iter: 24000\n",
      "D loss: 0.623\n",
      "G_loss: 2.947\n",
      "\n",
      "Iter: 25000\n",
      "D loss: 0.5264\n",
      "G_loss: 2.311\n",
      "\n",
      "Iter: 26000\n",
      "D loss: 0.4779\n",
      "G_loss: 3.064\n",
      "\n",
      "Iter: 27000\n",
      "D loss: 0.6544\n",
      "G_loss: 2.49\n",
      "\n",
      "Iter: 28000\n",
      "D loss: 0.4365\n",
      "G_loss: 2.752\n",
      "\n",
      "Iter: 29000\n",
      "D loss: 0.5899\n",
      "G_loss: 2.454\n",
      "\n",
      "Iter: 30000\n",
      "D loss: 0.6375\n",
      "G_loss: 2.651\n",
      "\n",
      "Iter: 31000\n",
      "D loss: 0.4795\n",
      "G_loss: 2.538\n",
      "\n",
      "Iter: 32000\n",
      "D loss: 0.4584\n",
      "G_loss: 2.801\n",
      "\n",
      "Iter: 33000\n",
      "D loss: 0.5634\n",
      "G_loss: 2.829\n",
      "\n",
      "Iter: 34000\n",
      "D loss: 0.6059\n",
      "G_loss: 2.705\n",
      "\n",
      "Iter: 35000\n",
      "D loss: 0.5599\n",
      "G_loss: 2.344\n",
      "\n",
      "Iter: 36000\n",
      "D loss: 0.6709\n",
      "G_loss: 2.451\n",
      "\n",
      "Iter: 37000\n",
      "D loss: 0.4934\n",
      "G_loss: 2.756\n",
      "\n",
      "Iter: 38000\n",
      "D loss: 0.5802\n",
      "G_loss: 2.659\n",
      "\n",
      "Iter: 39000\n",
      "D loss: 0.512\n",
      "G_loss: 2.569\n",
      "\n",
      "Iter: 40000\n",
      "D loss: 0.4997\n",
      "G_loss: 2.836\n",
      "\n",
      "Iter: 41000\n",
      "D loss: 0.5961\n",
      "G_loss: 2.701\n",
      "\n",
      "Iter: 42000\n",
      "D loss: 0.647\n",
      "G_loss: 2.533\n",
      "\n",
      "Iter: 43000\n",
      "D loss: 0.5981\n",
      "G_loss: 2.436\n",
      "\n",
      "Iter: 44000\n",
      "D loss: 0.4256\n",
      "G_loss: 2.793\n",
      "\n",
      "Iter: 45000\n",
      "D loss: 0.5493\n",
      "G_loss: 2.787\n",
      "\n",
      "Iter: 46000\n",
      "D loss: 0.5792\n",
      "G_loss: 2.84\n",
      "\n",
      "Iter: 47000\n",
      "D loss: 0.5463\n",
      "G_loss: 2.497\n",
      "\n",
      "Iter: 48000\n",
      "D loss: 0.5158\n",
      "G_loss: 2.462\n",
      "\n",
      "Iter: 49000\n",
      "D loss: 0.4533\n",
      "G_loss: 2.52\n",
      "\n",
      "Iter: 50000\n",
      "D loss: 0.5121\n",
      "G_loss: 2.37\n",
      "\n",
      "Iter: 51000\n",
      "D loss: 0.6848\n",
      "G_loss: 2.695\n",
      "\n",
      "Iter: 52000\n",
      "D loss: 0.5352\n",
      "G_loss: 2.367\n",
      "\n",
      "Iter: 53000\n",
      "D loss: 0.5791\n",
      "G_loss: 2.52\n",
      "\n",
      "Iter: 54000\n",
      "D loss: 0.4732\n",
      "G_loss: 2.144\n",
      "\n",
      "Iter: 55000\n",
      "D loss: 0.5239\n",
      "G_loss: 2.399\n",
      "\n",
      "Iter: 56000\n",
      "D loss: 0.6218\n",
      "G_loss: 2.726\n",
      "\n",
      "Iter: 57000\n",
      "D loss: 0.525\n",
      "G_loss: 2.419\n",
      "\n",
      "Iter: 58000\n",
      "D loss: 0.4942\n",
      "G_loss: 2.543\n",
      "\n",
      "Iter: 59000\n",
      "D loss: 0.6249\n",
      "G_loss: 2.804\n",
      "\n",
      "Iter: 60000\n",
      "D loss: 0.4441\n",
      "G_loss: 2.625\n",
      "\n",
      "Iter: 61000\n",
      "D loss: 0.6271\n",
      "G_loss: 2.503\n",
      "\n",
      "Iter: 62000\n",
      "D loss: 0.4653\n",
      "G_loss: 2.16\n",
      "\n",
      "Iter: 63000\n",
      "D loss: 0.5442\n",
      "G_loss: 2.5\n",
      "\n",
      "Iter: 64000\n",
      "D loss: 0.4102\n",
      "G_loss: 2.456\n",
      "\n",
      "Iter: 65000\n",
      "D loss: 0.555\n",
      "G_loss: 2.376\n",
      "\n",
      "Iter: 66000\n",
      "D loss: 0.5734\n",
      "G_loss: 2.537\n",
      "\n",
      "Iter: 67000\n",
      "D loss: 0.485\n",
      "G_loss: 2.442\n",
      "\n",
      "Iter: 68000\n",
      "D loss: 0.6185\n",
      "G_loss: 2.61\n",
      "\n",
      "Iter: 69000\n",
      "D loss: 0.568\n",
      "G_loss: 2.458\n",
      "\n",
      "Iter: 70000\n",
      "D loss: 0.4163\n",
      "G_loss: 2.802\n",
      "\n",
      "Iter: 71000\n",
      "D loss: 0.3832\n",
      "G_loss: 2.562\n",
      "\n",
      "Iter: 72000\n",
      "D loss: 0.4899\n",
      "G_loss: 2.721\n",
      "\n",
      "Iter: 73000\n",
      "D loss: 0.4236\n",
      "G_loss: 2.775\n",
      "\n",
      "Iter: 74000\n",
      "D loss: 0.5051\n",
      "G_loss: 2.426\n",
      "\n",
      "Iter: 75000\n",
      "D loss: 0.5962\n",
      "G_loss: 2.321\n",
      "\n",
      "Iter: 76000\n",
      "D loss: 0.3549\n",
      "G_loss: 2.896\n",
      "\n",
      "Iter: 77000\n",
      "D loss: 0.6002\n",
      "G_loss: 2.378\n",
      "\n",
      "Iter: 78000\n",
      "D loss: 0.5574\n",
      "G_loss: 2.681\n",
      "\n",
      "Iter: 79000\n",
      "D loss: 0.634\n",
      "G_loss: 2.615\n",
      "\n",
      "Iter: 80000\n",
      "D loss: 0.4143\n",
      "G_loss: 2.729\n",
      "\n",
      "Iter: 81000\n",
      "D loss: 0.4997\n",
      "G_loss: 2.499\n",
      "\n",
      "Iter: 82000\n",
      "D loss: 0.4807\n",
      "G_loss: 2.488\n",
      "\n",
      "Iter: 83000\n",
      "D loss: 0.4847\n",
      "G_loss: 2.439\n",
      "\n",
      "Iter: 84000\n",
      "D loss: 0.4043\n",
      "G_loss: 2.646\n",
      "\n",
      "Iter: 85000\n",
      "D loss: 0.5584\n",
      "G_loss: 2.765\n",
      "\n",
      "Iter: 86000\n",
      "D loss: 0.5291\n",
      "G_loss: 2.462\n",
      "\n",
      "Iter: 87000\n",
      "D loss: 0.4958\n",
      "G_loss: 2.619\n",
      "\n",
      "Iter: 88000\n",
      "D loss: 0.4678\n",
      "G_loss: 2.515\n",
      "\n",
      "Iter: 89000\n",
      "D loss: 0.5459\n",
      "G_loss: 2.771\n",
      "\n",
      "Iter: 90000\n",
      "D loss: 0.4536\n",
      "G_loss: 2.696\n",
      "\n",
      "Iter: 91000\n",
      "D loss: 0.4584\n",
      "G_loss: 2.77\n",
      "\n",
      "Iter: 92000\n",
      "D loss: 0.4282\n",
      "G_loss: 2.842\n",
      "\n",
      "Iter: 93000\n",
      "D loss: 0.5647\n",
      "G_loss: 3.095\n",
      "\n",
      "Iter: 94000\n",
      "D loss: 0.5859\n",
      "G_loss: 2.885\n",
      "\n",
      "Iter: 95000\n",
      "D loss: 0.5734\n",
      "G_loss: 2.583\n",
      "\n",
      "Iter: 96000\n",
      "D loss: 0.6006\n",
      "G_loss: 2.378\n",
      "\n",
      "Iter: 97000\n",
      "D loss: 0.538\n",
      "G_loss: 2.418\n",
      "\n",
      "Iter: 98000\n",
      "D loss: 0.5018\n",
      "G_loss: 2.5\n",
      "\n",
      "Iter: 99000\n",
      "D loss: 0.5718\n",
      "G_loss: 2.867\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "\n",
    "D_W1 = tf.Variable(xavier_init([784, 128]))\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "D_W2 = tf.Variable(xavier_init([128, 1]))\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[1]))\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_b1, D_b2]\n",
    "\n",
    "\n",
    "Z = tf.placeholder(tf.float32, shape=[None, 100])\n",
    "\n",
    "G_W1 = tf.Variable(xavier_init([100, 128]))\n",
    "G_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "G_W2 = tf.Variable(xavier_init([128, 784]))\n",
    "G_b2 = tf.Variable(tf.zeros(shape=[784]))\n",
    "\n",
    "theta_G = [G_W1, G_W2, G_b1, G_b2]\n",
    "\n",
    "\n",
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])\n",
    "\n",
    "\n",
    "def generator(z):\n",
    "    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n",
    "    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n",
    "    G_prob = tf.nn.sigmoid(G_log_prob)\n",
    "\n",
    "    return G_prob\n",
    "\n",
    "\n",
    "def discriminator(x):\n",
    "    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n",
    "    D_logit = tf.matmul(D_h1, D_W2) + D_b2\n",
    "    D_prob = tf.nn.sigmoid(D_logit)\n",
    "\n",
    "    return D_prob, D_logit\n",
    "\n",
    "\n",
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "G_sample = generator(Z)\n",
    "D_real, D_logit_real = discriminator(X)\n",
    "D_fake, D_logit_fake = discriminator(G_sample)\n",
    "\n",
    "# D_loss = -tf.reduce_mean(tf.log(D_real) + tf.log(1. - D_fake))\n",
    "# G_loss = -tf.reduce_mean(tf.log(D_fake))\n",
    "\n",
    "# Alternative losses:\n",
    "# -------------------\n",
    "D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))\n",
    "D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\n",
    "D_loss = D_loss_real + D_loss_fake\n",
    "G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake)))\n",
    "\n",
    "D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\n",
    "G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n",
    "\n",
    "mb_size = 128\n",
    "Z_dim = 100\n",
    "\n",
    "mnist = input_data.read_data_sets('../../MNIST_data', one_hot=True)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if not os.path.exists('out/'):\n",
    "    os.makedirs('out/')\n",
    "\n",
    "i = 0\n",
    "\n",
    "for it in range(100000):\n",
    "    if it % 1000 == 0:\n",
    "        samples = sess.run(G_sample, feed_dict={Z: sample_Z(16, Z_dim)})\n",
    "\n",
    "        fig = plot(samples)\n",
    "        plt.savefig('out/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n",
    "        i += 1\n",
    "        plt.close(fig)\n",
    "\n",
    "    X_mb, _ = mnist.train.next_batch(mb_size)\n",
    "\n",
    "    _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict={X: X_mb, Z: sample_Z(mb_size, Z_dim)})\n",
    "    _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={Z: sample_Z(mb_size, Z_dim)})\n",
    "\n",
    "    if it % 1000 == 0:\n",
    "        print('Iter: {}'.format(it))\n",
    "        print('D loss: {:.4}'. format(D_loss_curr))\n",
    "        print('G_loss: {:.4}'.format(G_loss_curr))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencias necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huama\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from keras.datasets import mnist #Dataset de Keras (modif de la implementacion base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicialización de pesos en las capas de la red\n",
    "\n",
    "En el modelo base se utiliza *Xavier initialization*. Este método de inicialización de pesos ayuda a que la varianza se mantenga similar en las diferentes capas la red neuronal. Esto evita que la magnitud de la señal de entrada a cada capa se dispare, o disminuya considerablemente. En el [paper original](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf), los autores proponen la siguiente fórmula:\n",
    "\n",
    "\n",
    " $$ Var(W_i) = \\frac{1}{N_i} $$\n",
    "\n",
    "Donde:\n",
    "- $ i $: Iterador que identifica cada capa de la red neuronal\n",
    "- $ W_i $: Pesos de las neuronas de la capa **\"i\"**\n",
    "- $ N_i $: Cantidad de neuronas de la capa **\"i\"**\n",
    "\n",
    "La varianza calculada para cada capa a partir de la fórmula arriba se utiliza para generar pesos aleatorios por capa, a partir de una distribución normal estándar:\n",
    "\n",
    "\\begin{equation}\n",
    "W_i \\sim \\mathcal{N}(\\mu = 0 , \\sigma = \\sqrt{\\frac{1}{N_i}})\n",
    "\\end{equation}\n",
    "\n",
    "Debido a que se utilizará una función de activación ReLU, se utilizará una modificación de la inicialización Xavier, la inicialización presentada por He et al. [2015](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf)  \n",
    "\n",
    "$$ W_i \\sim \\mathcal{N}(\\mu = 0 , \\sigma = \\sqrt{\\frac{2}{N_i}}) $$ \n",
    "\n",
    "\n",
    "La implementación en Python es la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de la arquitectura GAN:\n",
    "\n",
    "Inicialmente se utilizará 2 redes neuronales del tipo **perceptrón multicapa**, con 2 capas ocultas. En este sentido, se definen los siguientes elementos de Tensorflow:\n",
    "\n",
    "- *Placeholders*: Son las entradas de la red. Llámense **\"X\"** para la red discriminadora y **\"Z\"** para la red generadora\n",
    "- *Variables*: Son los parámetros que se modificarán a través del entrenamiento de la red. Se definen los pesos **\"W\"** y sesgos **\"b\"** para cada capa y red\n",
    "\n",
    "Finalmente, se almacenan los valores de los \"W\" y \"b\" en 2 listas, theta_D y theta_G (una para cada red):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de variables de entrada, salida y parámetros:\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "\n",
    "D_W1 = tf.Variable(xavier_init([784, 128]))\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "D_W2 = tf.Variable(xavier_init([128, 1]))\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[1]))\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_b1, D_b2]\n",
    "\n",
    "\n",
    "Z = tf.placeholder(tf.float32, shape=[None, 100])\n",
    "\n",
    "G_W1 = tf.Variable(xavier_init([100, 128]))\n",
    "G_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "G_W2 = tf.Variable(xavier_init([128, 784]))\n",
    "G_b2 = tf.Variable(tf.zeros(shape=[784]))\n",
    "\n",
    "theta_G = [G_W1, G_W2, G_b1, G_b2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de activación\n",
    "Se utiliza una función RELU (*Rectifying Linear Unit*) después de cada capa oculta. En la capa de salida, se utiliza una función sigmoidal para computar una probabilidad entre 0 y 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])\n",
    "\n",
    "\n",
    "def generator(z):\n",
    "    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n",
    "    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n",
    "    G_prob = tf.nn.sigmoid(G_log_prob)\n",
    "\n",
    "    return G_prob\n",
    "\n",
    "\n",
    "def discriminator(x):\n",
    "    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n",
    "    D_logit = tf.matmul(D_h1, D_W2) + D_b2\n",
    "    D_prob = tf.nn.sigmoid(D_logit)\n",
    "\n",
    "    return D_prob, D_logit\n",
    "\n",
    "\n",
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de pérdida\n",
    "\n",
    "La red Discriminadora y la red Generadora tienen una función de pérdida diferente:\n",
    "\n",
    "- Pérdida de la red Discriminadora: La denominaremos **D_loss**\n",
    "\\begin{equation}\n",
    "\\frac{1}{m} \\sum^m_{i=1} [ \\log D (x^{(i)}) + \\log (1 - D(G(z^{(i)})))]\n",
    "\\end{equation}\n",
    "\n",
    "- Pérdida de la red generadora: La denominaremos **G_loss**\n",
    "\\begin{equation}\n",
    "\\frac{1}{m} \\sum^m_{i=1} \\log (1 - D(G(z^{(i)})))\n",
    "\\end{equation}\n",
    "\n",
    "Se utilizarán las gradientes de dichas funciones por separado para entrenar una red a la vez, ya que no hay solución factible para la optimización en conjunto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo de entrenamiento:\n",
    "\n",
    "En la presente implementación, se utilizará el algoritmo de entrenamiento descrito por los autores del modelo, con ciertas modificaciones. Se define un número de iteraciones de entrenamiento y por cada iteración se aplicará una serie de operaciones sobre un **lote de \"m\" elementos**, las cuales llevarán a la actualización de los pesos y sesgos de las redes a través de la regla de aprendizaje **Adam** (Adaptive Momentum), la cual está basada en gradientes. Este método se conoce como *mini-batch gradient descent* y se detalla a continuación:\n",
    "\n",
    "**for** número de iteraciones de entrenamiento **do**\n",
    "1. Muestrear un lote de *m* resultados de la red Generadora (\"G\"), representados por el set $\\{z^{(1)},...,z^{(m)}\\}$\n",
    "1. Muestrear un lote de *m* elementos del conjunto de datos original (cuya distribución se desea modelar), representados por el set $\\{x^{(1)},...,x^{(m)}\\}$\n",
    "1. Introducir el set **z** y **x** como entradas a la red Discriminadora (\"D\")\n",
    "1. Actualizar los pesos y *bias* de la red Discriminadora (\"D\"), los cuales están almacenados como **theta_D** a través de la gradiente de su función de pérdida (**D_loss**) en función de **theta_D**\n",
    "\n",
    "1. Muestrear un nuevo lote de *m* resultados de la red Generadora (\"G\"), representados por el set $\\{z^{(1)},...,z^{(m)}\\}$\n",
    "1. Introducir el set **z** como entrada a la red Discriminadora (\"D\")\n",
    "1. Actualizar los pesos y *bias* de la red Generadora (\"G\"), los cuales están almacenados como **theta_G** a través de la gradiente de su función de pérdida (**G_loss**) en función de **theta_G**\n",
    "\n",
    "**end for**\n",
    "\n",
    "Nótese que primero se entrena a la red Discriminadora y luego a la red Generadora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_sample = generator(Z)\n",
    "D_real, D_logit_real = discriminator(X)\n",
    "D_fake, D_logit_fake = discriminator(G_sample)\n",
    "\n",
    "# Función de pérdida para la red generadora:\n",
    "D_loss = -tf.reduce_mean(tf.log(D_real) + tf.log(1. - D_fake))\n",
    "\n",
    "# Función de pérdida para la red discriminante:\n",
    "G_loss = -tf.reduce_mean(tf.log(D_fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\n",
    "G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-5e8b19462fef>:6: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\huama\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\huama\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\huama\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../../MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\huama\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../../MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\huama\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ../../MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ../../MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\huama\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Iter: 0\n",
      "D loss: 1.735\n",
      "G_loss: 2.011\n",
      "\n",
      "Iter: 1000\n",
      "D loss: 0.006165\n",
      "G_loss: 12.54\n",
      "\n",
      "Iter: 2000\n",
      "D loss: 0.04199\n",
      "G_loss: 5.998\n",
      "\n",
      "Iter: 3000\n",
      "D loss: 0.1062\n",
      "G_loss: 4.36\n",
      "\n",
      "Iter: 4000\n",
      "D loss: 0.08971\n",
      "G_loss: 6.945\n",
      "\n",
      "Iter: 5000\n",
      "D loss: 0.329\n",
      "G_loss: 4.857\n",
      "\n",
      "Iter: 6000\n",
      "D loss: 0.4057\n",
      "G_loss: 4.361\n",
      "\n",
      "Iter: 7000\n",
      "D loss: 0.5011\n",
      "G_loss: 3.226\n",
      "\n",
      "Iter: 8000\n",
      "D loss: 0.6767\n",
      "G_loss: 2.794\n",
      "\n",
      "Iter: 9000\n",
      "D loss: 0.4044\n",
      "G_loss: 3.69\n",
      "\n",
      "Iter: 10000\n",
      "D loss: 0.679\n",
      "G_loss: 2.812\n",
      "\n",
      "Iter: 11000\n",
      "D loss: 0.4968\n",
      "G_loss: 2.423\n",
      "\n",
      "Iter: 12000\n",
      "D loss: 0.7289\n",
      "G_loss: 2.155\n",
      "\n",
      "Iter: 13000\n",
      "D loss: 0.6908\n",
      "G_loss: 3.203\n",
      "\n",
      "Iter: 14000\n",
      "D loss: 0.8142\n",
      "G_loss: 2.516\n",
      "\n",
      "Iter: 15000\n",
      "D loss: 0.7133\n",
      "G_loss: 2.283\n",
      "\n",
      "Iter: 16000\n",
      "D loss: 0.6346\n",
      "G_loss: 2.184\n",
      "\n",
      "Iter: 17000\n",
      "D loss: 0.6718\n",
      "G_loss: 2.119\n",
      "\n",
      "Iter: 18000\n",
      "D loss: 0.8128\n",
      "G_loss: 2.158\n",
      "\n",
      "Iter: 19000\n",
      "D loss: 0.586\n",
      "G_loss: 2.178\n",
      "\n",
      "Iter: 20000\n",
      "D loss: 0.8254\n",
      "G_loss: 2.058\n",
      "\n",
      "Iter: 21000\n",
      "D loss: 0.6916\n",
      "G_loss: 2.352\n",
      "\n",
      "Iter: 22000\n",
      "D loss: 0.8258\n",
      "G_loss: 1.958\n",
      "\n",
      "Iter: 23000\n",
      "D loss: 0.8563\n",
      "G_loss: 1.896\n",
      "\n",
      "Iter: 24000\n",
      "D loss: 0.894\n",
      "G_loss: 2.017\n",
      "\n",
      "Iter: 25000\n",
      "D loss: 0.9271\n",
      "G_loss: 1.563\n",
      "\n",
      "Iter: 26000\n",
      "D loss: 0.8308\n",
      "G_loss: 2.0\n",
      "\n",
      "Iter: 27000\n",
      "D loss: 0.6855\n",
      "G_loss: 2.183\n",
      "\n",
      "Iter: 28000\n",
      "D loss: 0.9556\n",
      "G_loss: 1.648\n",
      "\n",
      "Iter: 29000\n",
      "D loss: 0.6729\n",
      "G_loss: 1.903\n",
      "\n",
      "Iter: 30000\n",
      "D loss: 0.7216\n",
      "G_loss: 1.978\n",
      "\n",
      "Iter: 31000\n",
      "D loss: 0.8223\n",
      "G_loss: 1.742\n",
      "\n",
      "Iter: 32000\n",
      "D loss: 0.7713\n",
      "G_loss: 2.027\n",
      "\n",
      "Iter: 33000\n",
      "D loss: 0.9071\n",
      "G_loss: 1.872\n",
      "\n",
      "Iter: 34000\n",
      "D loss: 0.7896\n",
      "G_loss: 1.924\n",
      "\n",
      "Iter: 35000\n",
      "D loss: 0.7704\n",
      "G_loss: 1.957\n",
      "\n",
      "Iter: 36000\n",
      "D loss: 0.7831\n",
      "G_loss: 2.129\n",
      "\n",
      "Iter: 37000\n",
      "D loss: 0.8066\n",
      "G_loss: 1.999\n",
      "\n",
      "Iter: 38000\n",
      "D loss: 0.7841\n",
      "G_loss: 1.806\n",
      "\n",
      "Iter: 39000\n",
      "D loss: 0.6697\n",
      "G_loss: 2.052\n",
      "\n",
      "Iter: 40000\n",
      "D loss: 0.6132\n",
      "G_loss: 2.195\n",
      "\n",
      "Iter: 41000\n",
      "D loss: 0.6811\n",
      "G_loss: 2.204\n",
      "\n",
      "Iter: 42000\n",
      "D loss: 0.8971\n",
      "G_loss: 1.867\n",
      "\n",
      "Iter: 43000\n",
      "D loss: 0.7247\n",
      "G_loss: 2.071\n",
      "\n",
      "Iter: 44000\n",
      "D loss: 0.7884\n",
      "G_loss: 1.962\n",
      "\n",
      "Iter: 45000\n",
      "D loss: 0.6606\n",
      "G_loss: 1.995\n",
      "\n",
      "Iter: 46000\n",
      "D loss: 0.7295\n",
      "G_loss: 2.097\n",
      "\n",
      "Iter: 47000\n",
      "D loss: 0.725\n",
      "G_loss: 1.889\n",
      "\n",
      "Iter: 48000\n",
      "D loss: 0.5636\n",
      "G_loss: 2.207\n",
      "\n",
      "Iter: 49000\n",
      "D loss: 0.9515\n",
      "G_loss: 2.131\n",
      "\n",
      "Iter: 50000\n",
      "D loss: 0.7399\n",
      "G_loss: 2.156\n",
      "\n",
      "Iter: 51000\n",
      "D loss: 0.803\n",
      "G_loss: 2.146\n",
      "\n",
      "Iter: 52000\n",
      "D loss: 0.7004\n",
      "G_loss: 2.154\n",
      "\n",
      "Iter: 53000\n",
      "D loss: 0.6061\n",
      "G_loss: 2.071\n",
      "\n",
      "Iter: 54000\n",
      "D loss: 0.6332\n",
      "G_loss: 2.122\n",
      "\n",
      "Iter: 55000\n",
      "D loss: 0.682\n",
      "G_loss: 2.166\n",
      "\n",
      "Iter: 56000\n",
      "D loss: 0.7452\n",
      "G_loss: 2.396\n",
      "\n",
      "Iter: 57000\n",
      "D loss: 0.6213\n",
      "G_loss: 1.997\n",
      "\n",
      "Iter: 58000\n",
      "D loss: 0.6849\n",
      "G_loss: 2.109\n",
      "\n",
      "Iter: 59000\n",
      "D loss: 0.8234\n",
      "G_loss: 1.766\n",
      "\n",
      "Iter: 60000\n",
      "D loss: 0.7207\n",
      "G_loss: 2.179\n",
      "\n",
      "Iter: 61000\n",
      "D loss: 0.7527\n",
      "G_loss: 2.014\n",
      "\n",
      "Iter: 62000\n",
      "D loss: 0.7878\n",
      "G_loss: 2.213\n",
      "\n",
      "Iter: 63000\n",
      "D loss: 0.7462\n",
      "G_loss: 2.387\n",
      "\n",
      "Iter: 64000\n",
      "D loss: 0.7559\n",
      "G_loss: 2.114\n",
      "\n",
      "Iter: 65000\n",
      "D loss: 0.6713\n",
      "G_loss: 2.191\n",
      "\n",
      "Iter: 66000\n",
      "D loss: 0.859\n",
      "G_loss: 2.315\n",
      "\n",
      "Iter: 67000\n",
      "D loss: 0.7957\n",
      "G_loss: 1.993\n",
      "\n",
      "Iter: 68000\n",
      "D loss: 0.7074\n",
      "G_loss: 2.375\n",
      "\n",
      "Iter: 69000\n",
      "D loss: 0.7752\n",
      "G_loss: 2.13\n",
      "\n",
      "Iter: 70000\n",
      "D loss: 0.6746\n",
      "G_loss: 2.268\n",
      "\n",
      "Iter: 71000\n",
      "D loss: 0.6991\n",
      "G_loss: 2.044\n",
      "\n",
      "Iter: 72000\n",
      "D loss: 0.8554\n",
      "G_loss: 2.334\n",
      "\n",
      "Iter: 73000\n",
      "D loss: 0.7092\n",
      "G_loss: 2.051\n",
      "\n",
      "Iter: 74000\n",
      "D loss: 0.6377\n",
      "G_loss: 2.072\n",
      "\n",
      "Iter: 75000\n",
      "D loss: 0.6791\n",
      "G_loss: 2.079\n",
      "\n",
      "Iter: 76000\n",
      "D loss: 0.6332\n",
      "G_loss: 2.214\n",
      "\n",
      "Iter: 77000\n",
      "D loss: 0.7437\n",
      "G_loss: 1.916\n",
      "\n",
      "Iter: 78000\n",
      "D loss: 0.6232\n",
      "G_loss: 2.309\n",
      "\n",
      "Iter: 79000\n",
      "D loss: 0.7295\n",
      "G_loss: 1.902\n",
      "\n",
      "Iter: 80000\n",
      "D loss: 0.6352\n",
      "G_loss: 2.182\n",
      "\n",
      "Iter: 81000\n",
      "D loss: 0.6015\n",
      "G_loss: 2.223\n",
      "\n",
      "Iter: 82000\n",
      "D loss: 0.6593\n",
      "G_loss: 2.149\n",
      "\n",
      "Iter: 83000\n",
      "D loss: 0.7334\n",
      "G_loss: 2.284\n",
      "\n",
      "Iter: 84000\n",
      "D loss: 0.7486\n",
      "G_loss: 1.986\n",
      "\n",
      "Iter: 85000\n",
      "D loss: 0.7158\n",
      "G_loss: 2.208\n",
      "\n",
      "Iter: 86000\n",
      "D loss: 0.6385\n",
      "G_loss: 2.442\n",
      "\n",
      "Iter: 87000\n",
      "D loss: 0.618\n",
      "G_loss: 2.56\n",
      "\n",
      "Iter: 88000\n",
      "D loss: 0.586\n",
      "G_loss: 2.425\n",
      "\n",
      "Iter: 89000\n",
      "D loss: 0.7377\n",
      "G_loss: 2.45\n",
      "\n",
      "Iter: 90000\n",
      "D loss: 0.6312\n",
      "G_loss: 2.06\n",
      "\n",
      "Iter: 91000\n",
      "D loss: 0.8371\n",
      "G_loss: 2.31\n",
      "\n",
      "Iter: 92000\n",
      "D loss: 0.6082\n",
      "G_loss: 2.522\n",
      "\n",
      "Iter: 93000\n",
      "D loss: 0.6597\n",
      "G_loss: 2.059\n",
      "\n",
      "Iter: 94000\n",
      "D loss: 0.5909\n",
      "G_loss: 2.512\n",
      "\n",
      "Iter: 95000\n",
      "D loss: 0.6949\n",
      "G_loss: 2.185\n",
      "\n",
      "Iter: 96000\n",
      "D loss: 0.5655\n",
      "G_loss: 2.536\n",
      "\n",
      "Iter: 97000\n",
      "D loss: 0.5911\n",
      "G_loss: 2.726\n",
      "\n",
      "Iter: 98000\n",
      "D loss: 0.6038\n",
      "G_loss: 2.485\n",
      "\n",
      "Iter: 99000\n",
      "D loss: 0.608\n",
      "G_loss: 2.386\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Usaremos el dataset MNIST:\n",
    "mb_size = 128\n",
    "Z_dim = 100\n",
    "num_iteraciones=100000\n",
    "\n",
    "mnist = input_data.read_data_sets('../../MNIST_data', one_hot=True)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if not os.path.exists('out/'):\n",
    "    os.makedirs('out/')\n",
    "\n",
    "i = 0\n",
    "\n",
    "for it in range(num_iteraciones):\n",
    "    if it % 1000 == 0:\n",
    "        samples = sess.run(G_sample, feed_dict={Z: sample_Z(16, Z_dim)})\n",
    "\n",
    "        fig = plot(samples)\n",
    "        plt.savefig('out/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n",
    "        i += 1\n",
    "        plt.close(fig)\n",
    "\n",
    "    X_mb, _ = mnist.train.next_batch(mb_size)\n",
    "\n",
    "    _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict={X: X_mb, Z: sample_Z(mb_size, Z_dim)})\n",
    "    _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={Z: sample_Z(mb_size, Z_dim)})\n",
    "\n",
    "    if it % 1000 == 0:\n",
    "        print('Iter: {}'.format(it))\n",
    "        print('D loss: {:.4}'. format(D_loss_curr))\n",
    "        print('G_loss: {:.4}'.format(G_loss_curr))\n",
    "        print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
